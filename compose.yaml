services:
  ollama-server:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped
  
  ollama-client:
    build:
      context: .
      dockerfile: Dockerfile.client
    depends_on:
      - ollama-server
    environment:
      - OLLAMA_HOST=http://ollama-server:11434
    command: >
      sh -c "
        echo 'Waiting for Ollama server to be ready...' &&
        while ! curl -s http://ollama-server:11434/api/tags > /dev/null; do
          sleep 5;
        done;
        echo 'Ollama server is ready. Pulling llama2 model... (This may take a while)';
        curl -X POST http://ollama-server:11434/api/pull -d '{\"name\": \"llama2\"}' || true;
        echo 'Running llama2 model with a simple prompt:';
        curl -X POST http://ollama-server:11434/api/generate -d '{
          \"model\": \"llama2\",
          \"prompt\": \"Why is the sky blue?\",
          \"stream\": false
        }'
      "

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    expose:
      - 9100
    networks:
      - monitoring

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    expose:
      - 9090
    networks:
      - monitoring


volumes:
  ollama_models:
  prometheus_data: {}