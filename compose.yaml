services:
  ollama-server-llava-phi:
    image: ollama/ollama:latest
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped

  ollama-server-moondream:
    image: ollama/ollama:latest
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped

  ollama-stats:
    build:
      context: .
      dockerfile: Dockerfile.stats
    depends_on:
      - ollama-server-llava-phi
      - ollama-server-moondream
    volumes:
      - /run/podman/podman.sock:/tmp/podman.sock

  ollama-client-llava-phi:
    build:
      context: .
      dockerfile: Dockerfile.client
    depends_on:
      - ollama-server-llava-phi
    environment:
      - OLLAMA_HOST=http://ollama-server-llava-phi:11434
    command: >
      sh -c "
        echo 'Waiting for Ollama server to be ready...' &&
        while ! curl -s http://ollama-server-llava-phi:11434/api/tags > /dev/null; do
          sleep 5;
        done;
        echo 'Ollama server is ready. Pulling llava-phi3:3.8b model... (This may take a while)';
        curl -X POST http://ollama-server-llava-phi:11434/api/pull -d '{\"name\": \"llava-phi3:3.8b\"}' || true;
        echo 'Running llava-phi3:4b model with a simple prompt:';
        curl -X POST http://ollama-server-llava-phi:11434/api/generate -d '{
          \"model\": \"llava-phi3:3.8b\",
          \"prompt\": \"For each picture in /app/parking_signs, tell me if I can park there and if so, for how long?\",
          \"stream\": false
        }'
      "

  ollama-client-moondream:
      build:
        context: .
        dockerfile: Dockerfile.client
      depends_on:
        - ollama-server-moondream
      environment:
        - OLLAMA_HOST=http://ollama-server-moondream:11434
      command: >
        sh -c "
          echo 'Waiting for Ollama server to be ready...' &&
          while ! curl -s http://ollama-server-moondream:11434/api/tags > /dev/null; do
            sleep 5;
          done;
          echo 'Ollama server is ready. Pulling moondream:1.8b model... (This may take a while)';
          curl -X POST http://ollama-server-moondream:11434/api/pull -d '{\"name\": \"moondream:1.8b\"}' || true;
          echo 'Running moondream:1.8b model with a simple prompt:';
          curl -X POST http://ollama-server-moondream:11434/api/generate -d '{
            \"model\": \"moondream:1.8b\",
            \"prompt\": \"For each picture in /app/parking_signs, tell me if I can park there and if so, for how long?\",
            \"stream\": false
          }'
        "

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    expose:
      - 9100
  
  grafana:
    build:
      context: .
      dockerfile: Dockerfile.grafana
    ports:
      - "3000:3000"

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      #- prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"


volumes:
  ollama_models:
  #prometheus_data: {}